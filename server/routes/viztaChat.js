const express = require('express');
const router = express.Router();
const { verifyUserAccess } = require('../middlewares/auth');
const mcpService = require('../services/mcp');
const recentScrapesService = require('../services/recentScrapes');
const memoriesService = require('../services/memories');
const agentesService = require('../services/agentesService');
const supabase = require('../utils/supabase');

// ===================================================================
// VIZTA CHAT ROUTES
// Endpoints para el chat inteligente con integraci√≥n MCP
// ===================================================================

/**
 * Post-procesa respuestas del chat para asegurar formato consistente
 */
function formatChatResponse(response, toolResult = null) {
  try {
    // Si es un objeto de respuesta modular, mantener su estructura
    if (typeof response === 'object' && response !== null) {
      // Si ya tiene la estructura correcta, devolverlo como est√°
      if (response.response && response.response.message) {
        return response;
      }
      
      // Si es una respuesta directa, darle la estructura correcta
      if (response.message) {
        return {
          success: true,
          response: {
            agent: response.agent || 'Vizta',
            message: response.message,
            type: response.type || 'chat_response',
            timestamp: response.timestamp || new Date().toISOString()
          },
          metadata: response.metadata || {}
        };
      }
    }

    // Si es string, aplicar el formateo
    let formattedText = response;

    // Limpiar respuesta muy larga
    if (formattedText && formattedText.length > 2000) {
      console.log('‚ö†Ô∏è Respuesta muy larga, truncando...');
      formattedText = formattedText.substring(0, 1800) + '\n\n*[Respuesta truncada para mejor legibilidad]*';
    }

    // Asegurar que tenga formato markdown b√°sico si no lo tiene
    if (typeof formattedText === 'string' && !formattedText.includes('##') && !formattedText.includes('###')) {
      const lines = formattedText.split('\n').filter(line => line.trim());
      
      if (lines.length > 0) {
        let formatted = `## üìä An√°lisis\n\n`;
        formatted += lines.join('\n\n');
        
        // Agregar resumen de datos si disponible
        if (toolResult && toolResult.tweets_found) {
          formatted += `\n\n### üìä Datos analizados:\n‚Ä¢ ${toolResult.tweets_found} tweets encontrados`;
          if (toolResult.analysis_metadata?.sentiment_distribution) {
            const sentiments = Object.entries(toolResult.analysis_metadata.sentiment_distribution);
            if (sentiments.length > 0) {
              formatted += `\n‚Ä¢ Sentimientos: ${sentiments.map(([s, c]) => `${s} (${c})`).join(', ')}`;
            }
          }
        }
        
        formattedText = formatted;
      }
    }

    // Si es string, limpiar formato
    if (typeof formattedText === 'string') {
      // Limpiar texto muy corrido (sin espacios entre p√°rrafos)
      formattedText = formattedText
        .replace(/\n{3,}/g, '\n\n') // M√°ximo 2 saltos de l√≠nea consecutivos
        .replace(/(\w)(\n)(### |## |\*\*)/g, '$1\n\n$3') // Espacios antes de headers
        .replace(/(\w)(\n)(‚Ä¢ )/g, '$1\n\n$3') // Espacios antes de bullets
        .trim();

      // Asegurar que los emojis tengan espacio despu√©s
      formattedText = formattedText.replace(/([üìäüìàüí≠‚ö°üéØüîç])([A-Za-z])/g, '$1 $2');
    }

    // Devolver con estructura est√°ndar
    return {
      success: true,
      response: {
        agent: 'Vizta',
        message: formattedText || 'No hay respuesta disponible',
        type: 'chat_response',
        timestamp: new Date().toISOString()
      },
      metadata: {
        formatted: true,
        hasToolResult: !!toolResult
      }
    };

  } catch (error) {
    console.error('‚ùå Error formateando respuesta:', error);
    return {
      success: false,
      response: {
        agent: 'Vizta',
        message: typeof response === 'string' ? response : response?.message || 'Error formateando respuesta',
        type: 'error',
        timestamp: new Date().toISOString()
      },
      metadata: {
        error: true,
        errorMessage: error.message
      }
    };
  }
}

// Cargar dependencias de forma condicional
let OpenAI, openai, uuidv4, openPipeService;

try {
  OpenAI = require('openai');
  const { v4 } = require('uuid');
  uuidv4 = v4;
  
  // Configurar OpenAI (para fallback)
  openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY
  });
  
  // Configurar OpenPipe Service
  openPipeService = require('../services/openPipeService');
  
  console.log('‚úÖ Dependencias de Vizta Chat cargadas correctamente');
  console.log('üéØ OpenPipe Service inicializado para function calling');
} catch (error) {
  console.warn('‚ö†Ô∏è Dependencias de Vizta Chat no disponibles:', error.message);
  console.warn('üì¶ Instala las dependencias con: npm install openai uuid');
}

/**
 * POST /api/vizta-chat/query
 * Endpoint principal para consultas de Vizta Chat
 */
router.post('/query', verifyUserAccess, async (req, res) => {
  try {
    // Verificar que las dependencias est√©n disponibles
    if (!openai || !uuidv4) {
      // Fallback temporal sin OpenAI
      console.log('‚ö†Ô∏è Usando fallback sin OpenAI para Vizta Chat');
      
      const fallbackSessionId = sessionId || `fallback_${Date.now()}`;
      
      // Usar directamente nitter_context como herramienta por defecto
      try {
        const toolResult = await mcpService.executeTool('nitter_context', {
          q: message,
          location: 'guatemala',
          limit: 25
        }, req.user);
        
        if (toolResult.success && toolResult.tweets) {
          // Guardar en recent_scrapes
          await recentScrapesService.saveScrape({
            queryOriginal: message,
            queryClean: message,
            herramienta: 'nitter_context',
            categoria: 'General',
            tweets: toolResult.tweets,
            userId: userId,
            sessionId: fallbackSessionId,
            mcpRequestId: `fallback_${Date.now()}`,
            mcpExecutionTime: 0,
            location: 'guatemala'
          });
          
          return res.json({
            success: true,
            response: `He encontrado ${toolResult.tweets.length} tweets relacionados con "${message}". Los datos han sido guardados y est√°n disponibles para an√°lisis.`,
            toolUsed: 'nitter_context',
            toolArgs: { q: message, location: 'guatemala', limit: 5 },
            toolResult: toolResult,
            sessionId: fallbackSessionId,
            requestId: `fallback_${Date.now()}`,
            executionTime: 0,
            timestamp: new Date().toISOString(),
            mode: 'fallback'
          });
        } else {
          throw new Error('No se pudieron obtener tweets');
        }
      } catch (error) {
        return res.status(500).json({
          success: false,
          message: 'Error en modo fallback: ' + error.message,
          error: 'Instala las dependencias con: npm run install-vizta'
        });
      }
    }

    const { message, sessionId } = req.body;
    const userId = req.user.id;
    
    if (!message) {
      return res.status(400).json({
        success: false,
        message: 'El mensaje es requerido'
      });
    }

    console.log(`ü§ñ Nueva consulta Vizta Chat de usuario ${userId}: "${message}"`);

    // Generar IDs √∫nicos
    const requestId = uuidv4();
    const chatSessionId = sessionId || uuidv4();

    // 1. Guardar mensaje del usuario en memories
    await memoriesService.saveMessage({
      sessionId: chatSessionId,
      userId: userId,
      role: 'user',
      content: message,
      messageType: 'message',
      modelUsed: 'gpt-4o-mini',
      metadata: { requestId: requestId }
    });

    // 2. Obtener los √∫ltimos 10 mensajes de la conversaci√≥n para contexto
    const conversationHistory = await memoriesService.getSessionMessages(chatSessionId, 10);
    const previousMessages = memoriesService.formatMessagesForOpenAI(conversationHistory);

    console.log('üéØ Iniciando orquestaci√≥n con sistema modular...');
    
    const startTime = Date.now();
    
    // PASO 1: Clasificar intenci√≥n con LLM
    const intentClassification = await classifyIntentWithLLM(message);
    console.log(`üß† Intenci√≥n detectada: ${intentClassification.intent} (${intentClassification.confidence})`);
    console.log(`üí≠ Razonamiento: ${intentClassification.reasoning}`);

    let result;
    
    if (intentClassification.intent === 'casual_chat') {
      // Manejar conversaci√≥n casual directamente
      result = {
        success: true,
        response: {
          agent: 'Vizta',
          message: await generateCasualResponse(message),
          type: 'casual_conversation',
          timestamp: new Date().toISOString()
        },
        conversationId: chatSessionId,
        metadata: {
          conversationType: 'casual',
          processingTime: Date.now() - startTime
        }
      };

      // Guardar respuesta casual en el historial
      await saveToHistory(req.user.id, message, result.response.message, chatSessionId);

    } else if (intentClassification.intent === 'codex_search') {
      // PASO 2: B√∫squeda en Codex
      result = await processCodexSearch(message, req.user, chatSessionId);
      
    } else if (intentClassification.intent === 'project_search') {
      // PASO 3: B√∫squeda en Proyectos
      result = await processProjectSearch(message, req.user, chatSessionId);
      
    } else {
      // PASO 3: Para consultas no casuales, usar OpenPipe con function calling
      console.log('üéØ Usando OpenPipe para function calling optimizado...');
      
      const openPipeResult = await openPipeService.processViztaQuery(message, req.user, chatSessionId);
      
      if (openPipeResult.success && openPipeResult.type === 'function_call') {
        // Ejecutar TODAS las herramientas decididas por el modelo en orden
        const toolCalls = Array.isArray(openPipeResult.allFunctionCalls) && openPipeResult.allFunctionCalls.length > 0
          ? openPipeResult.allFunctionCalls
          : [openPipeResult.functionCall];
        
        const toolExecutions = [];
        for (const call of toolCalls) {
          // Normalizar estructura de tool_call del SDK de OpenAI/OpenPipe
          const normalized = call.function
            ? { name: call.function.name, arguments: call.function.arguments }
            : call; // ya viene como { name, arguments }
          
          const toolName = normalized.name || 'desconocido';
          let argsObj = undefined;
          try {
            argsObj = typeof normalized.arguments === 'string'
              ? JSON.parse(normalized.arguments || '{}')
              : (normalized.arguments || {});
          } catch (e) {
            console.warn(`‚ö†Ô∏è No se pudo parsear arguments de ${toolName}:`, e.message);
          }

          console.log(`üîß Ejecutando funci√≥n: ${toolName}`);
          const exec = await openPipeService.executeFunctionCall(
            { name: toolName, arguments: argsObj },
            req.user,
            chatSessionId
          );
          toolExecutions.push({ call: normalized, exec });
          
          // Persistir breve rastro de cada ejecuci√≥n en memories
          try {
            await memoriesService.saveMessage({
              userId: req.user.id,
              sessionId: chatSessionId,
              role: 'assistant',
              content: exec?.success ? `‚úÖ Herramienta ${toolName} ejecutada` : `‚ùå Error ejecutando ${toolName}: ${exec?.error || 'desconocido'}`,
              messageType: 'function_result',
              metadata: {
                conversationType: 'function_call',
                functionCall: call,
                functionResult: exec
              }
            });
          } catch (e) {
            console.warn('‚ö†Ô∏è No se pudo guardar rastro de ejecuci√≥n en memories:', e.message);
          }
        }
        
        // Construir respuesta final priorizando la √∫ltima herramienta ejecutada
        const last = toolExecutions[toolExecutions.length - 1];
        const lastToolName = last?.call?.name || last?.call?.function?.name || 'desconocido';
        const lastExec = last?.exec || {};
        
        result = {
          success: lastExec.success,
          response: {
            agent: lastExec.agent || 'Vizta',
            message: await formatFunctionResult(lastExec, message),
            type: 'function_response',
            timestamp: new Date().toISOString(),
            functionUsed: lastToolName,
            data: lastExec.data
          },
          conversationId: chatSessionId,
          metadata: {
            conversationType: 'function_call',
            functionCalls: toolCalls,
            executions: toolExecutions.map(te => ({
              tool: te.call?.name || te.call?.function?.name,
              success: !!te.exec?.success
            })),
            agent: lastExec.agent,
            processingTime: Date.now() - startTime,
            openPipeUsage: openPipeResult.usage
          }
        };
        
      } else if (openPipeResult.success && openPipeResult.type === 'conversational') {
        // Respuesta conversacional directa de OpenPipe
        result = {
          success: true,
          response: {
            agent: 'Vizta',
            message: openPipeResult.message,
            type: 'conversational_ai',
            timestamp: new Date().toISOString()
          },
          conversationId: chatSessionId,
          metadata: {
            conversationType: 'ai_conversational',
            processingTime: Date.now() - startTime,
            openPipeUsage: openPipeResult.usage
          }
        };
        
      } else {
        // Fallback al sistema modular si OpenPipe falla
        console.log('‚ö†Ô∏è OpenPipe fall√≥, usando fallback al sistema modular');
        result = await agentesService.processUserQuery(message, req.user, {
          sessionId: chatSessionId,
          previousMessages: previousMessages
        });
      }
    }

    // Formatear respuesta para el chat y asegurar estructura correcta
    const finalResponse = formatChatResponse(result);

    // Guardar en historial si no es casual
    if (intentClassification.intent !== 'casual_chat') {
      // Extraer solo el texto del mensaje para guardar
      const messageToSave = typeof finalResponse.response?.message === 'string' 
        ? finalResponse.response.message 
        : JSON.stringify(finalResponse.response);

      await saveToHistory(req.user.id, message, messageToSave, chatSessionId);
    }
    
    return res.json(finalResponse);

  } catch (error) {
    console.error('‚ùå Error en consulta Vizta Chat:', error);
    return res.status(500).json({
      error: 'Error procesando consulta',
      details: error.message
    });
  }
});

// Funci√≥n helper para guardar en historial
async function saveToHistory(userId, message, response, sessionId) {
  try {
    await memoriesService.saveMessage({
      sessionId: sessionId,
      userId: userId,
      role: 'assistant',
      content: response,
      messageType: 'message',
      modelUsed: 'modular-system',
      metadata: {
        type: 'chat_response',
        timestamp: new Date().toISOString()
      }
    });
  } catch (error) {
    console.error('‚ùå Error guardando en historial:', error);
    // No lanzar el error para no interrumpir el flujo principal
  }
}

// Funci√≥n para formatear resultados de function calling
async function formatFunctionResult(functionResult, originalQuery) {
  try {
    if (!functionResult.success) {
      return `‚ùå Error ejecutando ${functionResult.tool}: ${functionResult.error}`;
    }

    const { agent, tool, data } = functionResult;
    
    switch (tool) {
      case 'nitter_context':
        if (data.tweets && data.tweets.length > 0) {
          return `üìä **An√°lisis de ${data.tweets.length} tweets sobre "${originalQuery}"**\n\n` +
                 `‚úÖ Encontr√© conversaciones relevantes en Twitter sobre este tema.\n` +
                 `üéØ **Tendencias detectadas:** ${data.tweets.slice(0, 3).map(t => t.text?.substring(0, 100) + '...').join('\n‚Ä¢ ')}\n\n` +
                 `üí° Los datos han sido procesados por ${agent} y est√°n disponibles para an√°lisis detallado.`;
        } else {
          return `üîç No se encontraron tweets recientes sobre "${originalQuery}". Intenta con t√©rminos diferentes o m√°s espec√≠ficos.`;
        }
        
      case 'nitter_profile':
        if (data.tweets && data.tweets.length > 0) {
          return `üë§ **Posts recientes de @${data.username || 'usuario'}**\n\n` +
                 `üìù √öltimos ${data.tweets.length} posts analizados por ${agent}.\n` +
                 `üóìÔ∏è Desde: ${new Date(data.tweets[data.tweets.length - 1]?.created_at || Date.now()).toLocaleDateString()}\n\n` +
                 `üí¨ **Contenido reciente:** ${data.tweets.slice(0, 2).map(t => `"${t.text?.substring(0, 150) + '...'}"`).join('\n‚Ä¢ ')}\n\n` +
                 `‚úÖ An√°lisis completo disponible en el sistema.`;
        } else {
          return `‚ùå No se pudieron obtener posts del perfil solicitado. El usuario podr√≠a tener perfil privado o no existir.`;
        }
        
      case 'perplexity_search':
        if (data.answer) {
          return `üîç **Informaci√≥n encontrada sobre "${originalQuery}"**\n\n${data.answer}\n\n` +
                 `üìö *Investigaci√≥n realizada por ${agent} usando fuentes web actualizadas.*`;
        } else {
          return `üîç B√∫squeda realizada por ${agent}, pero no se encontr√≥ informaci√≥n espec√≠fica sobre "${originalQuery}".`;
        }
        
      case 'search_political_context':
        if (data && data.length > 0) {
          return `üß† **Informaci√≥n en memoria pol√≠tica**\n\n` +
                 `‚úÖ Encontr√© ${data.length} resultado(s) relevante(s) en mi memoria sobre "${originalQuery}":\n\n` +
                 `${data.slice(0, 3).map((item, i) => `${i + 1}. ${item.substring(0, 200) + '...'}`).join('\n')}\n\n` +
                 `üí≠ *Datos recuperados por ${agent} desde la sesi√≥n pulse-politics.*`;
        } else {
          return `üß† Busqu√© en mi memoria pol√≠tica pero no encontr√© informaci√≥n espec√≠fica sobre "${originalQuery}". Podr√≠a estar en fuentes externas.`;
        }
        
      case 'resolve_twitter_handle':
        if (data.handle) {
          return `üîç **Handle encontrado:** @${data.handle}\n\n` +
                 `‚úÖ ${agent} identific√≥ la cuenta de Twitter asociada con "${originalQuery}".\n` +
                 `üì± Ahora puedo extraer sus posts si lo necesitas.`;
        } else {
          return `‚ùå No se pudo encontrar el handle de Twitter para "${originalQuery}". La persona podr√≠a no tener cuenta p√∫blica.`;
        }
        
      case 'user_projects':
        if (data.projects && data.projects.length > 0) {
          const activeProjects = data.projects.filter(p => p.status === 'active').length;
          return `üìã **Tus Proyectos**\n\n` +
                 `‚úÖ Tienes **${data.count}** proyecto(s) total, **${activeProjects}** activo(s).\n\n` +
                 `üéØ **Proyectos recientes:**\n${data.projects.slice(0, 5).map(p => `‚Ä¢ ${p.title} (${p.status})`).join('\n')}\n\n` +
                 `üìä *Datos gestionados por ${agent}.*`;
        } else {
          return `üìã No tienes proyectos registrados a√∫n. ¬øTe gustar√≠a que te ayude a crear uno?`;
        }
        
      case 'user_codex':
        if (data.items && data.items.length > 0) {
          return `üìö **Elementos encontrados en tu Codex**\n\n` +
                 `‚úÖ Encontr√© **${data.count}** elemento(s) relacionado(s) con "${originalQuery}":\n\n` +
                 `${data.items.slice(0, 5).map(item => `üìÑ ${item.titulo} (${item.tipo})\n   ${item.descripcion?.substring(0, 100) + '...' || 'Sin descripci√≥n'}`).join('\n\n')}\n\n` +
                 `üîç *B√∫squeda realizada por ${agent} en tu biblioteca personal.*`;
        } else {
          return `üìö No encontr√© elementos en tu Codex relacionados con "${originalQuery}". Intenta con t√©rminos diferentes.`;
        }
        
      default:
        return `‚úÖ ${agent} proces√≥ tu consulta usando ${tool}. Resultado disponible en el sistema.`;
    }
    
  } catch (error) {
    console.error('‚ùå Error formateando resultado de funci√≥n:', error);
    return `‚úÖ Funci√≥n ejecutada por ${functionResult.agent || 'Agente'}, pero hubo un error formateando la respuesta.`;
  }
}

// Funci√≥n para clasificar intenci√≥n con LLM
async function classifyIntentWithLLM(message) {
  const classificationPrompt = `
Analiza el siguiente mensaje del usuario y clasifica su intenci√≥n principal.

TIPOS DE INTENCI√ìN:
1. codex_search - Usuario quiere buscar, revisar o consultar algo espec√≠fico en el Codex
2. project_search - Usuario quiere buscar, revisar o consultar informaci√≥n sobre sus proyectos
3. agent_request - Usuario solicita un agente especializado o herramienta espec√≠fica  
4. casual_chat - Conversaci√≥n casual, saludo, charla general sin objetivo espec√≠fico
5. technical_help - Ayuda t√©cnica, programaci√≥n, configuraci√≥n
6. information_query - Pregunta informativa general

INSTRUCCIONES:
- Ignora el tono informal o formal del mensaje
- Enf√≥cate en la INTENCI√ìN REAL detr√°s de las palabras
- Si menciona "Codex" o quiere "revisar/ver/buscar algo en el codex", es codex_search
- Si menciona "proyectos", "proyecto", "tareas", "decisiones", "colaboradores", es project_search
- Si es solo saludo sin objetivo espec√≠fico, es casual_chat

Mensaje del usuario: "${message}"

Responde SOLO en este formato JSON:
{
  "intent": "tipo_de_intencion",
  "confidence": 0.95,
  "reasoning": "Breve explicaci√≥n de por qu√© clasificaste as√≠"
}`;

  try {
    const response = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: classificationPrompt }],
      temperature: 0.1,
      max_tokens: 150
    });

    const result = JSON.parse(response.choices[0].message.content);
    
    // Validaci√≥n b√°sica
    const validIntents = ['codex_search', 'project_search', 'agent_request', 'casual_chat', 'technical_help', 'information_query'];
    if (!validIntents.includes(result.intent)) {
      throw new Error('Intent no v√°lido recibido del LLM');
    }

    return result;
    
  } catch (error) {
    console.error('‚ùå Error en clasificaci√≥n de intenci√≥n:', error);
    // Fallback seguro
    return {
      intent: 'casual_chat',
      confidence: 0.5,
      reasoning: 'Error en clasificaci√≥n, usando fallback'
    };
  }
}

// Funci√≥n para procesar b√∫squedas en Codex con an√°lisis LLM
async function processCodexSearch(message, user, sessionId) {
  try {
    console.log('üîç Procesando b√∫squeda contextual en Codex...');
    
    // PASO 1: LLM genera t√©rminos de b√∫squeda contextuales con historial
    const conversationHistory = await memoriesService.getSessionMessages(sessionId, 5);
    const searchTerms = await generateContextualSearchTerms(message, conversationHistory);
    console.log(`üß† T√©rminos contextuales generados:`, searchTerms);
    
    // PASO 2: Buscar con m√∫ltiples t√©rminos contextuales y relaciones
    const codexResults = await searchCodexWithTerms(searchTerms, user.id, conversationHistory);
    console.log(`üìä Encontrados ${codexResults?.length || 0} resultados en Codex`);

    // PASO 3: LLM analiza relevancia de resultados
    if (codexResults && codexResults.length > 0) {
      const analysisResult = await analyzeCodexRelevance(message, codexResults);
      return analysisResult;
    }

    // Sin resultados
    return {
      success: true,
      response: {
        agent: 'Codex',
        message: `üîç **B√∫squeda contextual en Codex**\n\n‚ùå No encontr√© elementos relacionados con tu consulta en el Codex.\n\n**Tu consulta:** "${message}"\n**T√©rminos analizados:** ${searchTerms.join(', ')}\n\nüí° **Sugerencias:**\n‚Ä¢ Intenta con una descripci√≥n diferente\n‚Ä¢ Usa sin√≥nimos o t√©rminos relacionados\n‚Ä¢ Pregunta sobre temas m√°s espec√≠ficos`,
        type: 'codex_search',
        timestamp: new Date().toISOString()
      },
      metadata: {
        searchTerms: searchTerms,
        originalMessage: message,
        resultsCount: 0,
        userId: user.id,
        sessionId: sessionId
      }
    };
    
  } catch (error) {
    console.error('‚ùå Error en b√∫squeda contextual Codex:', error);
    return {
      success: false,
      response: {
        agent: 'Codex',
        message: `‚ùå **Error en b√∫squeda del Codex**\n\nNo pude completar la b√∫squeda debido a un error t√©cnico:\n${error.message}\n\nPor favor, intenta nuevamente.`,
        type: 'error',
        timestamp: new Date().toISOString()
      }
    };
  }
}

// Funci√≥n para generar t√©rminos de b√∫squeda contextuales con LLM
async function generateContextualSearchTerms(userQuery, conversationHistory = []) {
  const contextMessages = conversationHistory
    .slice(-5) // √∫ltimos 5 mensajes
    .map(msg => `${msg.role}: ${msg.content}`)
    .join('\n');

  const searchPrompt = `
Analiza la siguiente consulta del usuario y genera t√©rminos de b√∫squeda contextuales inteligentes.

CONSULTA ACTUAL: "${userQuery}"

HISTORIAL DE CONVERSACI√ìN RECIENTE:
${contextMessages}

INSTRUCCIONES:
- Si el usuario usa referencias como "ese proyecto", "eso", "lo anterior", usa el historial para identificar a qu√© se refiere espec√≠ficamente
- Identifica el tema/concepto principal que busca
- Genera 3-5 t√©rminos de b√∫squeda relacionados basados en el contexto completo
- Incluye sin√≥nimos, t√©rminos relacionados y variaciones
- NO uses palabras vac√≠as como "hola", "revisame", "tengo", "algo de"
- Enf√≥cate en sustantivos, conceptos y t√©rminos espec√≠ficos
- Si hay nombres de proyectos o conceptos espec√≠ficos mencionados antes, incl√∫yelos

EJEMPLO:
Historial: "user: cu√°ntos proyectos tengo? assistant: Tienes 3 proyectos: Marketing Digital (activo)..."
Consulta: "dime elementos del codex de ese proyecto"
T√©rminos: ["Marketing Digital", "marketing", "digital", "proyecto marketing", "elementos codex"]

Responde SOLO con un array JSON de t√©rminos:
["t√©rmino1", "t√©rmino2", "t√©rmino3"]`;

  try {
    const response = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: searchPrompt }],
      temperature: 0.3,
      max_tokens: 300
    });

    let responseText = response.choices[0].message.content.trim();
    
    // Limpiar formato markdown si existe
    responseText = responseText.replace(/```json\n?/g, '').replace(/```\n?/g, '');
    responseText = responseText.replace(/`/g, '');
    
    const termsArray = JSON.parse(responseText);
    return Array.isArray(termsArray) ? termsArray : [userQuery];
    
  } catch (error) {
    console.error('‚ùå Error generando t√©rminos contextuales:', error);
    // Fallback b√°sico
    return [userQuery.replace(/[¬ø?]/g, '').trim()];
  }
}

// Funci√≥n para buscar en Codex con m√∫ltiples t√©rminos y relaciones
async function searchCodexWithTerms(searchTerms, userId, conversationHistory = []) {
  try {
    const allResults = [];
    
    // PASO 1: Intentar b√∫squeda relacional inteligente basada en contexto
    const relationalResults = await performRelationalCodexSearch(searchTerms, userId, conversationHistory);
    if (relationalResults.length > 0) {
      console.log(`üîó Encontrados ${relationalResults.length} resultados relacionales`);
      allResults.push(...relationalResults);
    }
    
    // PASO 2: B√∫squeda tradicional por t√©rminos
    for (const term of searchTerms) {
      const { data: results } = await supabase
        .from('codex_items')
        .select(`
          id, titulo, descripcion, tipo, etiquetas, proyecto, project_id,
          audio_transcription, document_analysis, created_at, fecha
        `)
        .eq('user_id', userId)
        .or(`titulo.ilike.%${term}%,descripcion.ilike.%${term}%,audio_transcription.ilike.%${term}%,document_analysis.ilike.%${term}%`)
        .limit(6);
        
      if (results) {
        allResults.push(...results);
      }
    }
    
    // Eliminar duplicados y ordenar
    const uniqueResults = allResults.filter((item, index, self) => 
      index === self.findIndex(i => i.id === item.id)
    );
    
    return uniqueResults
      .sort((a, b) => new Date(b.created_at) - new Date(a.created_at))
      .slice(0, 15);
      
  } catch (error) {
    console.error('‚ùå Error en b√∫squeda con t√©rminos:', error);
    return [];
  }
}

// Funci√≥n para b√∫squeda relacional inteligente en Codex
async function performRelationalCodexSearch(searchTerms, userId, conversationHistory) {
  try {
    console.log('üîó Intentando b√∫squeda relacional...');
    
    // Detectar si hay referencia a proyectos en el historial
    const projectContext = await extractProjectContext(conversationHistory, searchTerms);
    
    if (projectContext.projectName) {
      console.log(`üéØ Proyecto identificado: ${projectContext.projectName}`);
      
      // Buscar el proyecto espec√≠fico
      const { data: project } = await supabase
        .from('projects')
        .select('id, title')
        .eq('user_id', userId)
        .ilike('title', `%${projectContext.projectName}%`)
        .single();
        
      if (project) {
        console.log(`‚úÖ Proyecto encontrado: ${project.title} (${project.id})`);
        
        // Buscar elementos del codex asociados a este proyecto
        const { data: codexItems } = await supabase
          .from('codex_items')
          .select(`
            id, titulo, descripcion, tipo, etiquetas, proyecto, project_id,
            audio_transcription, document_analysis, created_at, fecha
          `)
          .eq('user_id', userId)
          .eq('project_id', project.id)
          .limit(10);
          
        if (codexItems && codexItems.length > 0) {
          console.log(`üéâ Encontrados ${codexItems.length} elementos del codex para proyecto ${project.title}`);
          return codexItems;
        }
      }
    }
    
    return [];
    
  } catch (error) {
    console.error('‚ùå Error en b√∫squeda relacional:', error);
    return [];
  }
}

// Funci√≥n para extraer contexto de proyecto del historial
async function extractProjectContext(conversationHistory, searchTerms) {
  const contextPrompt = `
Analiza el historial de conversaci√≥n y los t√©rminos de b√∫squeda para identificar si hay una referencia espec√≠fica a un proyecto.

HISTORIAL:
${conversationHistory.map(msg => `${msg.role}: ${msg.content}`).join('\n')}

T√âRMINOS DE B√öSQUEDA: ${searchTerms.join(', ')}

INSTRUCCIONES:
- Busca nombres espec√≠ficos de proyectos mencionados en el historial
- Si hay referencia a "ese proyecto", "mi proyecto", identifica cu√°l proyecto espec√≠fico
- Extrae el nombre exacto del proyecto si est√° disponible
- Si no hay referencia clara a un proyecto espec√≠fico, devuelve null

Responde SOLO en formato JSON:
{
  "projectName": "nombre exacto del proyecto o null",
  "confidence": 0.0-1.0,
  "reasoning": "explicaci√≥n breve"
}`;

  try {
    const response = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: contextPrompt }],
      temperature: 0.2,
      max_tokens: 150
    });

    let responseText = response.choices[0].message.content.trim();
    
    // Limpiar formato markdown si existe
    responseText = responseText.replace(/```json\n?/g, '').replace(/```\n?/g, '');
    responseText = responseText.replace(/`/g, '');
    
    console.log('üîç Respuesta del LLM para contexto:', responseText);
    
    const context = JSON.parse(responseText);
    return context;
    
  } catch (error) {
    console.error('‚ùå Error extrayendo contexto de proyecto:', error);
    console.error('‚ùå Respuesta problem√°tica:', response?.choices?.[0]?.message?.content);
    return { projectName: null, confidence: 0, reasoning: 'Error en an√°lisis' };
  }
}

// Funci√≥n para analizar relevancia con LLM
async function analyzeCodexRelevance(userQuery, codexResults) {
  const analysisPrompt = `
Responde de forma simple y directa sobre estos elementos del Codex del usuario.

CONSULTA: "${userQuery}"

ELEMENTOS ENCONTRADOS:
${codexResults.map((item, index) => `
${index + 1}. ${item.titulo} (${item.tipo})
   ${item.descripcion || ''}
   ${item.audio_transcription ? `Transcripci√≥n: ${item.audio_transcription.substring(0, 300)}...` : ''}
   ${item.document_analysis ? `An√°lisis: ${item.document_analysis.substring(0, 300)}...` : ''}
`).join('\n')}

INSTRUCCIONES:
- Responde en m√°ximo 3-4 l√≠neas
- S√© directo: "Encontr√© X elementos sobre Y"
- Menciona solo lo m√°s relevante
- Tono casual y amigable
- NO hagas an√°lisis extensos ni explicaciones largas

Ejemplo: "Encontr√© 2 elementos sobre LGBT. Hay una transcripci√≥n que habla de AESDI y la lucha estudiantil LGBT, y un enlace que parece no funcionar."

Responde de forma simple:`;

  try {
    const response = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: analysisPrompt }],
      temperature: 0.3,
      max_tokens: 400
    });

    return {
      success: true,
      response: {
        agent: 'Codex',
        message: response.choices[0].message.content,
        type: 'codex_search',
        timestamp: new Date().toISOString()
      },
      metadata: {
        originalMessage: userQuery,
        resultsCount: codexResults.length,
        results: codexResults
      }
    };
    
  } catch (error) {
    console.error('‚ùå Error en an√°lisis de relevancia:', error);
    
    // Fallback con formato b√°sico
    let message = `üîç **B√∫squeda en Codex**\n\n‚úÖ Encontr√© **${codexResults.length}** elemento(s) relacionado(s):\n\n`;
    
    codexResults.slice(0, 5).forEach((item, index) => {
      message += `**${index + 1}. ${item.titulo}**\n`;
      message += `   üìÇ ${item.tipo} ‚Ä¢ üìÖ ${new Date(item.created_at).toLocaleDateString()}\n`;
      if (item.descripcion) {
        message += `   üìù ${item.descripcion.substring(0, 150)}...\n`;
      }
      message += `\n`;
    });
    
    return {
      success: true,
      response: {
        agent: 'Codex',
        message: message,
        type: 'codex_search',
        timestamp: new Date().toISOString()
      },
      metadata: {
        originalMessage: userQuery,
        resultsCount: codexResults.length,
        results: codexResults
      }
    };
  }
}

// Funci√≥n para procesar b√∫squedas en Proyectos con an√°lisis LLM
async function processProjectSearch(message, user, sessionId) {
  try {
    console.log('üîç Procesando b√∫squeda contextual en Proyectos...');
    
    // PASO 1: LLM analiza el tipo de consulta sobre proyectos
    const queryAnalysis = await analyzeProjectQueryType(message);
    console.log(`üß† An√°lisis de consulta:`, queryAnalysis);
    
    if (queryAnalysis.type === 'statistics') {
      // Manejo especial para consultas de estad√≠sticas
      const statsResult = await processProjectStatsWithLLM(message, user.id, queryAnalysis);
      return statsResult;
    }
    
    // PASO 2: LLM genera t√©rminos de b√∫squeda contextuales para proyectos con historial
    const conversationHistory = await memoriesService.getSessionMessages(sessionId, 5);
    const searchTerms = await generateProjectSearchTerms(message, conversationHistory);
    console.log(`üß† T√©rminos de proyecto generados:`, searchTerms);
    
    // PASO 3: Buscar en m√∫ltiples tablas de proyectos
    const projectResults = await searchProjectsWithTerms(searchTerms, user.id);
    console.log(`üìä Encontrados ${projectResults?.totalResults || 0} resultados en Proyectos`);

    // PASO 3: LLM analiza relevancia de resultados de proyectos
    if (projectResults && projectResults.totalResults > 0) {
      const analysisResult = await analyzeProjectRelevance(message, projectResults);
      return analysisResult;
    }

    // Sin resultados
    return {
      success: true,
      response: {
        agent: 'Projects',
        message: `üîç **B√∫squeda en Proyectos**\n\n‚ùå No encontr√© informaci√≥n relacionada con tu consulta en tus proyectos.\n\n**Tu consulta:** "${message}"\n**T√©rminos analizados:** ${searchTerms.join(', ')}\n\nüí° **Sugerencias:**\n‚Ä¢ Intenta con el nombre espec√≠fico del proyecto\n‚Ä¢ Pregunta sobre tareas, decisiones o colaboradores\n‚Ä¢ Usa t√©rminos m√°s generales`,
        type: 'project_search',
        timestamp: new Date().toISOString()
      },
      metadata: {
        searchTerms: searchTerms,
        originalMessage: message,
        resultsCount: 0,
        userId: user.id,
        sessionId: sessionId
      }
    };
    
  } catch (error) {
    console.error('‚ùå Error en b√∫squeda contextual Proyectos:', error);
    return {
      success: false,
      response: {
        agent: 'Projects',
        message: `‚ùå **Error en b√∫squeda de Proyectos**\n\nNo pude completar la b√∫squeda debido a un error t√©cnico:\n${error.message}\n\nPor favor, intenta nuevamente.`,
        type: 'error',
        timestamp: new Date().toISOString()
      }
    };
  }
}

// Funci√≥n para analizar tipo de consulta sobre proyectos con LLM
async function analyzeProjectQueryType(message) {
  const analysisPrompt = `
Analiza la siguiente consulta del usuario sobre proyectos y determina el tipo de respuesta necesaria.

CONSULTA: "${message}"

TIPOS DE CONSULTA:
1. "statistics" - Usuario quiere estad√≠sticas, conteos, res√∫menes o informaci√≥n cuantitativa sobre sus proyectos
2. "search" - Usuario busca proyectos espec√≠ficos por nombre, contenido, caracter√≠sticas o filtros
3. "management" - Usuario quiere crear, editar, eliminar o gestionar proyectos
4. "details" - Usuario quiere detalles espec√≠ficos de un proyecto particular

EJEMPLOS:
- "cu√°ntos proyectos tengo" ‚Üí statistics
- "mis proyectos activos" ‚Üí statistics  
- "busca proyecto Guatemala" ‚Üí search
- "proyecto de marketing" ‚Üí search
- "crea nuevo proyecto" ‚Üí management
- "detalles del proyecto X" ‚Üí details

INSTRUCCIONES:
- Analiza la intenci√≥n real, no solo palabras clave
- Si pide conteos, totales, listas completas ‚Üí statistics
- Si busca algo espec√≠fico por caracter√≠sticas ‚Üí search
- Responde en JSON con el tipo y explicaci√≥n breve

Responde SOLO en formato JSON:
{
  "type": "statistics|search|management|details",
  "reasoning": "explicaci√≥n breve de por qu√©",
  "focus": "qu√© aspecto espec√≠fico busca el usuario"
}`;

  try {
    const response = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: analysisPrompt }],
      temperature: 0.2,
      max_tokens: 200
    });

    let responseText = response.choices[0].message.content.trim();
    
    // Limpiar formato markdown si existe
    responseText = responseText.replace(/```json\n?/g, '').replace(/```\n?/g, '');
    responseText = responseText.replace(/`/g, '');
    
    const analysis = JSON.parse(responseText);
    return analysis;
    
  } catch (error) {
    console.error('‚ùå Error analizando tipo de consulta:', error);
    return {
      type: 'search',
      reasoning: 'Error en an√°lisis, usando b√∫squeda por defecto',
      focus: 'general'
    };
  }
}

// Funci√≥n para procesar estad√≠sticas de proyectos con an√°lisis LLM
async function processProjectStatsWithLLM(message, userId, queryAnalysis) {
  try {
    console.log('üìä Obteniendo estad√≠sticas de proyectos...');
    
    // Obtener conteos de todas las tablas de proyectos
    const [projectsCount, contextsCount, decisionsCount, coveragesCount, cardsCount] = await Promise.all([
      // Contar proyectos
      supabase.from('projects').select('id', { count: 'exact' }).eq('user_id', userId),
      
      // Contar contextos  
      supabase.from('project_contexts')
        .select('id', { count: 'exact' })
        .eq('projects.user_id', userId),
        
      // Contar decisiones
      supabase.from('project_decisions')
        .select('id', { count: 'exact' })
        .eq('projects.user_id', userId),
        
      // Contar coberturas
      supabase.from('project_coverages')
        .select('id', { count: 'exact' })
        .eq('projects.user_id', userId),
        
      // Contar hallazgos
      supabase.from('capturado_cards')
        .select('id', { count: 'exact' })
        .eq('projects.user_id', userId)
    ]);

    // Tambi√©n obtener proyectos con detalles para estad√≠sticas adicionales
    const { data: projectsData } = await supabase
      .from('projects')
      .select('id, title, status, priority, category, created_at')
      .eq('user_id', userId);

    const stats = {
      totalProjects: projectsCount.count || 0,
      totalContexts: contextsCount.count || 0, 
      totalDecisions: decisionsCount.count || 0,
      totalCoverages: coveragesCount.count || 0,
      totalCards: cardsCount.count || 0,
      projects: projectsData || []
    };

    // Estad√≠sticas por estado
    const statusStats = {};
    if (projectsData) {
      projectsData.forEach(p => {
        statusStats[p.status] = (statusStats[p.status] || 0) + 1;
      });
    }

    // Generar respuesta con LLM usando an√°lisis contextual
    const responseMessage = await generateStatsResponseWithContext(message, stats, statusStats, queryAnalysis);
    
    return {
      success: true,
      response: {
        agent: 'Projects',
        message: responseMessage,
        type: 'project_stats',
        timestamp: new Date().toISOString()
      },
      metadata: {
        originalMessage: message,
        stats: stats,
        statusBreakdown: statusStats,
        userId: userId
      }
    };
    
  } catch (error) {
    console.error('‚ùå Error obteniendo estad√≠sticas de proyectos:', error);
    return {
      success: false,
      response: {
        agent: 'Projects',
        message: `‚ùå Error obteniendo estad√≠sticas de proyectos: ${error.message}`,
        type: 'error',
        timestamp: new Date().toISOString()
      }
    };
  }
}

// Funci√≥n para generar respuesta de estad√≠sticas con contexto LLM
async function generateStatsResponseWithContext(userQuery, stats, statusStats, queryAnalysis) {
  const statsPrompt = `
El usuario pregunta sobre estad√≠sticas de sus proyectos. Usa el an√°lisis de contexto para personalizar tu respuesta.

CONSULTA: "${userQuery}"
AN√ÅLISIS DE CONTEXTO: ${queryAnalysis.reasoning}
ENFOQUE ESPEC√çFICO: ${queryAnalysis.focus}

ESTAD√çSTICAS DISPONIBLES:
- Total de proyectos: ${stats.totalProjects}
- Contextos de proyecto: ${stats.totalContexts}
- Decisiones registradas: ${stats.totalDecisions}
- Coberturas geogr√°ficas: ${stats.totalCoverages}
- Hallazgos/descubrimientos: ${stats.totalCards}

PROYECTOS POR ESTADO:
${Object.entries(statusStats).map(([status, count]) => `- ${status}: ${count}`).join('\n')}

PROYECTOS RECIENTES:
${stats.projects.slice(0, 3).map(p => `- "${p.title}" (${p.status}) - creado ${new Date(p.created_at).toLocaleDateString()}`).join('\n')}

INSTRUCCIONES:
- Responde bas√°ndote en lo que espec√≠ficamente pregunta el usuario
- Usa el an√°lisis de contexto para enfocar tu respuesta
- M√°ximo 3-4 l√≠neas, tono casual y directo
- Si pregunta cantidad espec√≠fica, da el n√∫mero exacto
- Si quiere resumen general, incluye breakdown por estado
- Menciona proyectos espec√≠ficos si es relevante

Responde de forma simple y contextual:`;

  try {
    const response = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: statsPrompt }],
      temperature: 0.3,
      max_tokens: 300
    });

    return response.choices[0].message.content;
    
  } catch (error) {
    console.error('‚ùå Error generando respuesta de estad√≠sticas:', error);
    
    // Fallback b√°sico
    return `üìä **Estad√≠sticas de Proyectos**\n\nTienes **${stats.totalProjects}** proyecto(s) en total.\n\n**Por estado:** ${Object.entries(statusStats).map(([s, c]) => `${s} (${c})`).join(', ')}\n\n**Otros elementos:** ${stats.totalDecisions} decisiones, ${stats.totalCards} hallazgos`;
  }
}

// Funci√≥n para generar t√©rminos de b√∫squeda contextuales para proyectos
async function generateProjectSearchTerms(userQuery, conversationHistory = []) {
  const contextMessages = conversationHistory
    .slice(-5) // √∫ltimos 5 mensajes
    .map(msg => `${msg.role}: ${msg.content}`)
    .join('\n');

  const searchPrompt = `
Analiza la siguiente consulta del usuario sobre PROYECTOS y genera t√©rminos de b√∫squeda contextuales.

CONSULTA ACTUAL: "${userQuery}"

HISTORIAL DE CONVERSACI√ìN RECIENTE:
${contextMessages}

CONTEXTO: El usuario quiere buscar informaci√≥n sobre sus proyectos personales, que pueden incluir:
- Nombres y t√≠tulos de proyectos
- Estados (activo, pausado, completado, archivado)  
- Tareas y decisiones del proyecto
- Colaboradores y roles
- Categor√≠as y etiquetas
- Ubicaciones geogr√°ficas
- Fechas y plazos
- Hallazgos y descubrimientos

INSTRUCCIONES:
- Si usa referencias como "ese proyecto", "mi proyecto anterior", usa el historial para identificar el proyecto espec√≠fico
- Identifica qu√© aspecto de los proyectos busca (nombre, estado, tareas, etc.)
- Genera 3-5 t√©rminos de b√∫squeda relacionados basados en el contexto completo
- Incluye sin√≥nimos y variaciones
- NO uses palabras vac√≠as como "mis", "tengo", "mu√©strame"
- Enf√≥cate en sustantivos y conceptos espec√≠ficos de proyectos
- Si hay nombres espec√≠ficos de proyectos mencionados antes, incl√∫yelos

EJEMPLO:
Historial: "assistant: Tienes 3 proyectos: Marketing Digital (activo), An√°lisis Guatemala..."
Consulta: "elementos del codex de ese proyecto"
T√©rminos: ["Marketing Digital", "marketing", "digital", "elementos", "codex"]

Responde SOLO con un array JSON de t√©rminos:
["t√©rmino1", "t√©rmino2", "t√©rmino3"]`;

  try {
    const response = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: searchPrompt }],
      temperature: 0.3,
      max_tokens: 250
    });

    let responseText = response.choices[0].message.content.trim();
    
    // Limpiar formato markdown si existe
    responseText = responseText.replace(/```json\n?/g, '').replace(/```\n?/g, '');
    responseText = responseText.replace(/`/g, '');
    
    const termsArray = JSON.parse(responseText);
    return Array.isArray(termsArray) ? termsArray : [userQuery];
    
  } catch (error) {
    console.error('‚ùå Error generando t√©rminos de proyecto:', error);
    return [userQuery.replace(/[¬ø?]/g, '').trim()];
  }
}

// Funci√≥n para buscar en todas las tablas de proyectos
async function searchProjectsWithTerms(searchTerms, userId) {
  try {
    const allResults = {
      projects: [],
      contexts: [],
      decisions: [],
      coverages: [],
      memory: [],
      cards: [],
      totalResults: 0
    };
    
    // Buscar en cada tabla con cada t√©rmino contextual
    for (const term of searchTerms) {
      console.log(`üîé Buscando "${term}" en tablas de proyectos...`);
      
      // 1. Buscar en projects
      const { data: projects } = await supabase
        .from('projects')
        .select('id, title, description, status, priority, category, tags, start_date, target_date, created_at')
        .eq('user_id', userId)
        .or(`title.ilike.%${term}%,description.ilike.%${term}%,category.ilike.%${term}%`)
        .limit(5);
        
      if (projects) allResults.projects.push(...projects);

      // 2. Buscar en project_contexts
      const { data: contexts } = await supabase
        .from('project_contexts')
        .select(`
          id, project_id, situation_description, main_problem, 
          geographic_scope, time_frame, context_type, version,
          projects!inner(title)
        `)
        .eq('projects.user_id', userId)
        .or(`situation_description.ilike.%${term}%,main_problem.ilike.%${term}%,geographic_scope.ilike.%${term}%`)
        .limit(3);
        
      if (contexts) allResults.contexts.push(...contexts);

      // 3. Buscar en project_decisions
      const { data: decisions } = await supabase
        .from('project_decisions')
        .select(`
          id, project_id, title, description, decision_type, status,
          rationale, expected_impact, urgency, created_at,
          projects!inner(title)
        `)
        .eq('projects.user_id', userId)
        .or(`title.ilike.%${term}%,description.ilike.%${term}%,rationale.ilike.%${term}%`)
        .limit(3);
        
      if (decisions) allResults.decisions.push(...decisions);

      // 4. Buscar en project_coverages
      const { data: coverages } = await supabase
        .from('project_coverages')
        .select(`
          id, project_id, coverage_type, name, parent_name, 
          description, relevance, topic, created_at,
          projects!inner(title)
        `)
        .eq('projects.user_id', userId)
        .or(`name.ilike.%${term}%,description.ilike.%${term}%,topic.ilike.%${term}%`)
        .limit(3);
        
      if (coverages) allResults.coverages.push(...coverages);

      // 5. Buscar en capturado_cards (hallazgos)
      const { data: cards } = await supabase
        .from('capturado_cards')
        .select(`
          id, project_id, entity, city, department, pais, 
          discovery, title, topic, description, created_at,
          projects!inner(title)
        `)
        .eq('projects.user_id', userId)
        .or(`entity.ilike.%${term}%,discovery.ilike.%${term}%,title.ilike.%${term}%,topic.ilike.%${term}%`)
        .limit(3);
        
      if (cards) allResults.cards.push(...cards);
    }
    
    // Eliminar duplicados de cada categor√≠a
    ['projects', 'contexts', 'decisions', 'coverages', 'cards'].forEach(category => {
      allResults[category] = allResults[category].filter((item, index, self) => 
        index === self.findIndex(i => i.id === item.id)
      );
    });
    
    // Calcular total de resultados
    allResults.totalResults = Object.values(allResults)
      .filter(val => Array.isArray(val))
      .reduce((total, arr) => total + arr.length, 0);
    
    return allResults;
      
  } catch (error) {
    console.error('‚ùå Error en b√∫squeda de proyectos:', error);
    return { totalResults: 0 };
  }
}

// Funci√≥n para analizar relevancia de proyectos con LLM
async function analyzeProjectRelevance(userQuery, projectResults) {
  const analysisPrompt = `
Responde de forma simple y directa sobre estos elementos de proyectos del usuario.

CONSULTA: "${userQuery}"

ELEMENTOS ENCONTRADOS:

PROYECTOS (${projectResults.projects.length}):
${projectResults.projects.map(p => `- ${p.title} (${p.status}) - ${p.description || 'Sin descripci√≥n'}`).join('\n')}

CONTEXTOS (${projectResults.contexts.length}):
${projectResults.contexts.map(c => `- ${c.projects.title}: ${c.situation_description?.substring(0, 100)}...`).join('\n')}

DECISIONES (${projectResults.decisions.length}):
${projectResults.decisions.map(d => `- ${d.projects.title}: ${d.title} (${d.status})`).join('\n')}

COBERTURA GEOGR√ÅFICA (${projectResults.coverages.length}):
${projectResults.coverages.map(c => `- ${c.projects.title}: ${c.name} (${c.coverage_type})`).join('\n')}

HALLAZGOS (${projectResults.cards.length}):
${projectResults.cards.map(c => `- ${c.projects.title}: ${c.title || c.discovery}`).join('\n')}

INSTRUCCIONES:
- Responde en m√°ximo 4-5 l√≠neas
- S√© directo: "Encontr√© X proyectos sobre Y"
- Menciona solo lo m√°s relevante
- Tono casual y amigable
- Agrupa la informaci√≥n por proyecto cuando sea posible

Ejemplo: "Encontr√© 2 proyectos relacionados: 'Marketing Digital' (activo) con 3 decisiones pendientes, y 'An√°lisis Guatemala' con hallazgos en 5 ciudades."

Responde de forma simple:`;

  try {
    const response = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: analysisPrompt }],
      temperature: 0.3,
      max_tokens: 400
    });

    return {
      success: true,
      response: {
        agent: 'Projects',
        message: response.choices[0].message.content,
        type: 'project_search',
        timestamp: new Date().toISOString()
      },
      metadata: {
        originalMessage: userQuery,
        resultsCount: projectResults.totalResults,
        results: projectResults
      }
    };
    
  } catch (error) {
    console.error('‚ùå Error en an√°lisis de relevancia de proyectos:', error);
    
    // Fallback con formato b√°sico
    let message = `üîç **B√∫squeda en Proyectos**\n\n‚úÖ Encontr√© **${projectResults.totalResults}** elemento(s) relacionado(s):\n\n`;
    
    if (projectResults.projects.length > 0) {
      message += `**Proyectos (${projectResults.projects.length}):**\n`;
      projectResults.projects.slice(0, 3).forEach(p => {
        message += `‚Ä¢ ${p.title} (${p.status})\n`;
      });
      message += `\n`;
    }
    
    if (projectResults.decisions.length > 0) {
      message += `**Decisiones (${projectResults.decisions.length}):**\n`;
      projectResults.decisions.slice(0, 2).forEach(d => {
        message += `‚Ä¢ ${d.title} - ${d.projects.title}\n`;
      });
    }
    
    return {
      success: true,
      response: {
        agent: 'Projects',
        message: message,
        type: 'project_search',
        timestamp: new Date().toISOString()
      },
      metadata: {
        originalMessage: userQuery,
        resultsCount: projectResults.totalResults,
        results: projectResults
      }
    };
  }
}

// Funci√≥n helper para generar respuestas casuales
async function generateCasualResponse(message) {
  const responses = {
    greetings: [
      "¬°Hola! üëã Soy Vizta, tu asistente inteligente. ¬øEn qu√© puedo ayudarte hoy?",
      "¬°Hola! üòä Me alegra verte. Estoy aqu√≠ para ayudarte con an√°lisis social y gesti√≥n de datos.",
      "¬°Bienvenido! üåü Soy Vizta, y junto con Laura y Robert podemos ayudarte con an√°lisis de tendencias y m√°s.",
      "¬°Hola! ü§ñ ¬øQu√© te gustar√≠a analizar hoy?"
    ],
    howAreYou: [
      "¬°Muy bien, gracias! üí™ Mis sistemas est√°n funcionando perfectamente. ¬øEn qu√© puedo ayudarte?",
      "¬°Excelente! üöÄ Laura y Robert est√°n listos para cualquier an√°lisis que necesites.",
      "¬°Todo perfecto! üòä Listo para ayudarte con an√°lisis social, tendencias o tus proyectos personales."
    ]
  };

  const msgLower = message.toLowerCase();
  let responseArray;

  if (msgLower.includes('qu√© tal') || msgLower.includes('como estas') || msgLower.includes('que tal')) {
    responseArray = responses.howAreYou;
  } else {
    responseArray = responses.greetings;
  }

  const randomIndex = Math.floor(Math.random() * responseArray.length);
  return responseArray[randomIndex];
}

/**
 * GET /api/vizta-chat/scrapes
 * Obtener scrapes del usuario
 */
router.get('/scrapes', verifyUserAccess, async (req, res) => {
  try {
    const userId = req.user.id;
    const { limit, offset, herramienta, categoria, sessionId } = req.query;

    const scrapes = await recentScrapesService.getUserScrapes(userId, {
      limit: parseInt(limit) || 20,
      offset: parseInt(offset) || 0,
      herramienta,
      categoria,
      sessionId
    });

    res.json({
      success: true,
      scrapes: scrapes,
      count: scrapes.length
    });

  } catch (error) {
    console.error('‚ùå Error obteniendo scrapes:', error);
    res.status(500).json({
      success: false,
      message: 'Error obteniendo scrapes',
      error: error.message
    });
  }
});

/**
 * GET /api/vizta-chat/stats
 * Obtener estad√≠sticas de scrapes del usuario
 */
router.get('/stats', verifyUserAccess, async (req, res) => {
  try {
    const userId = req.user.id;
    const stats = await recentScrapesService.getUserScrapeStats(userId);

    res.json({
      success: true,
      stats: stats
    });

  } catch (error) {
    console.error('‚ùå Error obteniendo estad√≠sticas:', error);
    res.status(500).json({
      success: false,
      message: 'Error obteniendo estad√≠sticas',
      error: error.message
    });
  }
});

/**
 * GET /api/vizta-chat/session/:sessionId
 * Obtener scrapes de una sesi√≥n espec√≠fica
 */
router.get('/session/:sessionId', verifyUserAccess, async (req, res) => {
  try {
    const { sessionId } = req.params;
    const scrapes = await recentScrapesService.getSessionScrapes(sessionId);

    res.json({
      success: true,
      scrapes: scrapes,
      sessionId: sessionId,
      count: scrapes.length
    });

  } catch (error) {
    console.error('‚ùå Error obteniendo scrapes de sesi√≥n:', error);
    res.status(500).json({
      success: false,
      message: 'Error obteniendo scrapes de sesi√≥n',
      error: error.message
    });
  }
});

/**
 * GET /api/vizta-chat/tools
 * Obtener herramientas MCP disponibles
 */
router.get('/tools', verifyUserAccess, async (req, res) => {
  try {
    const tools = await mcpService.listAvailableTools();

    res.json({
      success: true,
      tools: tools,
      count: tools.length
    });

  } catch (error) {
    console.error('‚ùå Error obteniendo herramientas MCP:', error);
    res.status(500).json({
      success: false,
      message: 'Error obteniendo herramientas',
      error: error.message
    });
  }
});

/**
 * GET /api/vizta-chat/conversations
 * Obtener lista de conversaciones del usuario
 */
router.get('/conversations', verifyUserAccess, async (req, res) => {
  try {
    const userId = req.user.id;
    const { limit } = req.query;

    const sessions = await memoriesService.getUserSessions(userId, parseInt(limit) || 20);

    res.json({
      success: true,
      conversations: sessions,
      count: sessions.length
    });

  } catch (error) {
    console.error('‚ùå Error obteniendo conversaciones:', error);
    res.status(500).json({
      success: false,
      message: 'Error obteniendo conversaciones',
      error: error.message
    });
  }
});

/**
 * GET /api/vizta-chat/conversation/:sessionId
 * Obtener mensajes de una conversaci√≥n espec√≠fica
 */
router.get('/conversation/:sessionId', verifyUserAccess, async (req, res) => {
  try {
    const { sessionId } = req.params;
    const { limit } = req.query;

    const messages = await memoriesService.getSessionMessages(sessionId, parseInt(limit) || 50);

    res.json({
      success: true,
      messages: messages,
      sessionId: sessionId,
      count: messages.length
    });

  } catch (error) {
    console.error('‚ùå Error obteniendo mensajes de conversaci√≥n:', error);
    res.status(500).json({
      success: false,
      message: 'Error obteniendo mensajes de conversaci√≥n',
      error: error.message
    });
  }
});

/**
 * GET /api/vizta-chat/memory-stats
 * Obtener estad√≠sticas de uso de memoria del usuario
 */
router.get('/memory-stats', verifyUserAccess, async (req, res) => {
  try {
    const userId = req.user.id;
    const stats = await memoriesService.getUserMemoryStats(userId);

    res.json({
      success: true,
      stats: stats
    });

  } catch (error) {
    console.error('‚ùå Error obteniendo estad√≠sticas de memoria:', error);
    res.status(500).json({
      success: false,
      message: 'Error obteniendo estad√≠sticas de memoria',
      error: error.message
    });
  }
});

/**
 * DELETE /api/vizta-chat/conversation/:sessionId
 * Eliminar una conversaci√≥n completa
 */
router.delete('/conversation/:sessionId', verifyUserAccess, async (req, res) => {
  try {
    const { sessionId } = req.params;
    const userId = req.user.id;

    // Verificar que la sesi√≥n pertenece al usuario
    const messages = await memoriesService.getSessionMessages(sessionId, 1);
    if (messages.length === 0 || messages[0].user_id !== userId) {
      return res.status(404).json({
        success: false,
        message: 'Conversaci√≥n no encontrada'
      });
    }

    // Eliminar todos los mensajes de la sesi√≥n
    const { error } = await supabase
      .from('memories')
      .delete()
      .eq('session_id', sessionId)
      .eq('user_id', userId);

    if (error) {
      throw error;
    }

    res.json({
      success: true,
      message: 'Conversaci√≥n eliminada exitosamente',
      sessionId: sessionId
    });

  } catch (error) {
    console.error('‚ùå Error eliminando conversaci√≥n:', error);
    res.status(500).json({
      success: false,
      message: 'Error eliminando conversaci√≥n',
      error: error.message
    });
  }
});

/**
 * POST /api/vizta-chat/test-expansion
 * Endpoint de prueba para probar la expansi√≥n inteligente de t√©rminos
 */
router.post('/test-expansion', verifyUserAccess, async (req, res) => {
  try {
    const { query } = req.body;
    
    if (!query || typeof query !== 'string' || query.trim().length === 0) {
      return res.status(400).json({
        success: false,
        message: 'El par√°metro "query" es requerido y debe ser un string no vac√≠o'
      });
    }

    // Obtener herramientas disponibles del MCP
    const availableTools = await mcpService.listAvailableTools();
    
    // Simular el proceso de expansi√≥n que har√≠a GPT-4o mini
    const originalQuery = query.trim();
    
    // Usar las funciones de expansi√≥n del MCP para mostrar c√≥mo funcionar√≠an
    console.log(`üß™ Prueba de expansi√≥n para: "${originalQuery}"`);
    
    // Crear un prompt de ejemplo mostrando c√≥mo GPT-4o mini deber√≠a procesar
    const examplePrompt = `USUARIO: "${originalQuery}"

AN√ÅLISIS ESTRAT√âGICO:
1. T√©rminos detectados: ${originalQuery.toLowerCase().split(' ').join(', ')}
2. Contexto inferido: Guatemala, redes sociales
3. Tipo de consulta: ${originalQuery.toLowerCase().includes('sentimiento') || originalQuery.toLowerCase().includes('opinion') ? 'An√°lisis de sentimiento' : 'B√∫squeda de contenido'}

EXPANSI√ìN SUGERIDA:
- Original: "${originalQuery}"
- Expandido: [Se simular√≠a la expansi√≥n aqu√≠]
- Hashtags probables: #Guatemala, #GuatemalaGt
- T√©rminos relacionados: [Se agregar√≠an t√©rminos espec√≠ficos]
- L√≠mite recomendado: ${originalQuery.toLowerCase().includes('sentimiento') ? '20-25 tweets' : '15 tweets'}

HERRAMIENTAS A USAR:
- nitter_context con par√°metros optimizados
- location: guatemala
- limit: optimizado seg√∫n tipo de consulta`;

    res.json({
      success: true,
      test_results: {
        original_query: originalQuery,
        analysis_type: originalQuery.toLowerCase().includes('sentimiento') || originalQuery.toLowerCase().includes('opinion') ? 'sentiment_analysis' : 'content_search',
        suggested_improvements: {
          should_expand_terms: true,
          should_include_hashtags: true,
          should_add_guatemalan_context: true,
          recommended_limit: originalQuery.toLowerCase().includes('sentimiento') ? 20 : 15
        },
        example_prompt: examplePrompt,
        available_tools: availableTools.map(tool => ({
          name: tool.name,
          description: tool.description,
          optimizations_applied: tool.name === 'nitter_context' ? [
            'Expansi√≥n inteligente de t√©rminos',
            'Optimizaci√≥n autom√°tica de l√≠mites',
            'Contexto guatemalteco a√±adido',
            'An√°lisis de sentimiento incluido'
          ] : []
        }))
      },
      instructions: {
        next_steps: [
          'El sistema ahora expandir√° autom√°ticamente los t√©rminos de b√∫squeda',
          'GPT-4o mini usar√° estrategias inteligentes en lugar de t√©rminos literales',
          'Los l√≠mites se optimizar√°n seg√∫n el tipo de an√°lisis',
          'Se incluir√° contexto guatemalteco autom√°ticamente'
        ],
        example_expansions: {
          'marcha del orgullo': 'Orgullo2025 OR MarchadelOrgullo OR OrguIIoGt OR Pride OR LGBTI OR diversidad',
          'elecciones': 'EleccionesGt OR TSE OR voto OR candidatos OR Elecciones2025 OR procesoelectoral',
          'presidente': 'BernardoArevalo OR presidente OR GobiernoGt OR CasaPresidencial OR Presidencia'
        }
      }
    });

  } catch (error) {
    console.error('‚ùå Error en test de expansi√≥n:', error);
    res.status(500).json({
      success: false,
      message: 'Error probando expansi√≥n de t√©rminos',
      error: error.message
    });
  }
});

/**
 * GET /api/vizta-chat/scrapes/grouped
 * Obtener scrapes agrupados inteligentemente
 */
router.get('/scrapes/grouped', verifyUserAccess, async (req, res) => {
  try {
    const userId = req.user.id;
    const { limit, offset, detectedGroup, categoria } = req.query;

    const groupedScrapes = await recentScrapesService.getGroupedScrapes(userId, {
      limit: parseInt(limit) || 20,
      offset: parseInt(offset) || 0,
      detectedGroup,
      categoria
    });

    res.json({
      success: true,
      groups: groupedScrapes,
      count: groupedScrapes.length,
      metadata: {
        totalGroups: groupedScrapes.length,
        requestedAt: new Date().toISOString()
      }
    });

  } catch (error) {
    console.error('‚ùå Error obteniendo scrapes agrupados:', error);
    res.status(500).json({
      success: false,
      message: 'Error obteniendo scrapes agrupados',
      error: error.message
    });
  }
});

/**
 * GET /api/vizta-chat/scrapes/grouped-stats
 * Obtener estad√≠sticas de agrupaci√≥n
 */
router.get('/scrapes/grouped-stats', verifyUserAccess, async (req, res) => {
  try {
    const userId = req.user.id;
    const stats = await recentScrapesService.getGroupedStats(userId);

    res.json({
      success: true,
      stats: stats,
      generatedAt: new Date().toISOString()
    });

  } catch (error) {
    console.error('‚ùå Error obteniendo estad√≠sticas agrupadas:', error);
    res.status(500).json({
      success: false,
      message: 'Error obteniendo estad√≠sticas agrupadas',
      error: error.message
    });
  }
});

/**
 * POST /api/vizta-chat/test-user-discovery
 * Endpoint de prueba para verificar User Discovery
 */
router.post('/test-user-discovery', verifyUserAccess, async (req, res) => {
  try {
    const { message } = req.body;
    
    if (!message) {
      return res.status(400).json({
        success: false,
        message: 'Par√°metro "message" es requerido'
      });
    }

    console.log(`üß™ TEST USER DISCOVERY: "${message}"`);

    // Procesar el mensaje usando el sistema completo
    const result = await agentesService.processUserQuery(message, req.user);

    res.json({
      success: true,
      test_input: message,
      intent_detected: result.metadata?.intent,
      intent_confidence: result.metadata?.intentConfidence,
      intent_method: result.metadata?.intentMethod,
      mode: result.metadata?.mode,
      agent_response: result.response?.agent,
      processing_time: result.metadata?.processingTime,
      user_discovery_activated: result.metadata?.intent === 'user_discovery',
      response_preview: result.response?.message?.substring(0, 200) + '...',
      full_result: process.env.NODE_ENV === 'development' ? result : undefined,
      timestamp: new Date().toISOString()
    });

  } catch (error) {
    console.error('‚ùå Error en test user discovery:', error);
    res.status(500).json({
      success: false,
      message: 'Error probando user discovery',
      error: error.message
    });
  }
});

/**
 * POST /api/vizta-chat/test-openpipe
 * Endpoint de prueba para verificar integraci√≥n con OpenPipe
 */
router.post('/test-openpipe', verifyUserAccess, async (req, res) => {
  try {
    if (!openPipeService) {
      return res.status(503).json({
        success: false,
        message: 'OpenPipe service no disponible',
        error: 'Dependencias no cargadas'
      });
    }

    const { message } = req.body;
    
    if (!message) {
      return res.status(400).json({
        success: false,
        message: 'Par√°metro "message" es requerido'
      });
    }

    console.log(`üß™ TEST OPENPIPE: "${message}"`);

    const startTime = Date.now();
    
    // Procesar con OpenPipe
    const openPipeResult = await openPipeService.processViztaQuery(message, req.user, 'test_session');
    
    let testResult = {
      success: true,
      test_input: message,
      openpipe_result: openPipeResult,
      processing_time: Date.now() - startTime,
      timestamp: new Date().toISOString()
    };

    // Si hay function call, ejecutarlo
    if (openPipeResult.success && openPipeResult.type === 'function_call') {
      console.log(`üîß Ejecutando funci√≥n de prueba: ${openPipeResult.functionCall.name}`);
      
      const functionResult = await openPipeService.executeFunctionCall(
        openPipeResult.functionCall,
        req.user,
        'test_session'
      );
      
      testResult.function_execution = functionResult;
      testResult.formatted_response = await formatFunctionResult(functionResult, message);
    }

    res.json(testResult);

  } catch (error) {
    console.error('‚ùå Error en test OpenPipe:', error);
    res.status(500).json({
      success: false,
      message: 'Error probando OpenPipe',
      error: error.message
    });
  }
});

/**
 * DELETE /api/vizta-chat/scrapes/:scrapeId
 * Eliminar un scrape espec√≠fico del usuario
 */
router.delete('/scrapes/:scrapeId', verifyUserAccess, async (req, res) => {
  try {
    const { scrapeId } = req.params;
    const userId = req.user.id;

    // Validar par√°metros
    if (!scrapeId || scrapeId.trim() === '') {
      return res.status(400).json({
        success: false,
        message: 'ID del scrape es requerido'
      });
    }

    console.log(`üóëÔ∏è Solicitud de eliminaci√≥n de scrape ${scrapeId} por usuario ${userId}`);

    // Eliminar scrape usando el servicio
    const result = await recentScrapesService.deleteScrape(scrapeId.trim(), userId);

    res.json({
      success: true,
      message: result.message,
      deletedScrape: result.deletedScrape,
      deletedAt: new Date().toISOString()
    });

  } catch (error) {
    console.error('‚ùå Error eliminando scrape:', error);
    
    // Manejar errores espec√≠ficos
    if (error.message.includes('no encontrado') || error.message.includes('no tienes permisos')) {
      return res.status(404).json({
        success: false,
        message: error.message
      });
    }

    res.status(500).json({
      success: false,
      message: 'Error eliminando scrape',
      error: error.message
    });
  }
});

module.exports = router;
