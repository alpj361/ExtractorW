/**
 * Clasificador de intenciones basado en LLM
 * Usa OpenAI para entender la intenci√≥n del usuario con mayor precisi√≥n
 */

const OpenAI = require('openai');

class LLMIntentClassifier {
  constructor() {
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY
    });
    
    this.model = 'gpt-3.5-turbo';
    this.maxTokens = 50;
    this.temperature = 0.1; // Baja temperatura para respuestas consistentes
  }

  /**
   * Prompt principal para clasificaci√≥n de intenciones
   */
  getClassificationPrompt(message) {
    return `Eres un clasificador de intenciones para un asistente de IA llamado Vizta.

Clasifica el siguiente mensaje del usuario en UNA de estas categor√≠as exactas:

MODO CONVERSACIONAL (respuesta directa de Vizta):
- "casual_conversation" ‚Üí saludos, despedidas, conversaci√≥n casual
- "capability_question" ‚Üí preguntas sobre qu√© puede hacer el asistente
- "help_request" ‚Üí solicitudes de ayuda general o comandos
- "small_talk" ‚Üí charla casual, chistes, comentarios generales

MODO AG√âNTICO (delegar a agentes especializados):
- "nitter_search" ‚Üí buscar en Twitter/X, hashtags, trending topics
- "twitter_analysis" ‚Üí analizar sentimientos, tendencias en Twitter
- "twitter_profile" ‚Üí buscar perfiles espec√≠ficos en Twitter
- "web_search" ‚Üí investigar informaci√≥n general en internet
- "search_codex" ‚Üí buscar en documentos personales/codex del usuario
- "search_projects" ‚Üí consultar proyectos activos del usuario
- "analyze_document" ‚Üí analizar documentos espec√≠ficos
- "mixed_analysis" ‚Üí tareas que requieren m√∫ltiples agentes

ESPECIALES:
- "unknown" ‚Üí si no encaja en ninguna categor√≠a anterior

Mensaje del usuario: "${message}"

Responde SOLO con la categor√≠a correspondiente (ejemplo: "casual_conversation").`;
  }

  /**
   * Clasifica la intenci√≥n usando OpenAI
   */
  async classifyIntent(message, conversationHistory = []) {
    try {
      console.log(`[LLM_CLASSIFIER] üß† Clasificando: "${message}"`);
      
      const startTime = Date.now();
      
      const response = await this.openai.chat.completions.create({
        model: this.model,
        messages: [
          {
            role: 'system',
            content: this.getClassificationPrompt(message)
          },
          {
            role: 'user',
            content: message
          }
        ],
        max_tokens: this.maxTokens,
        temperature: this.temperature
      });

      const intent = response.choices[0]?.message?.content?.trim();
      const processingTime = Date.now() - startTime;
      
      console.log(`[LLM_CLASSIFIER] ‚úÖ Intenci√≥n detectada: "${intent}" (${processingTime}ms)`);
      
      // Validar que la respuesta sea una de las categor√≠as v√°lidas
      const validIntents = [
        'casual_conversation', 'capability_question', 'help_request', 'small_talk',
        'nitter_search', 'twitter_analysis', 'twitter_profile', 'web_search',
        'search_codex', 'search_projects', 'analyze_document', 'mixed_analysis',
        'unknown'
      ];
      
      if (!validIntents.includes(intent)) {
        console.warn(`[LLM_CLASSIFIER] ‚ö†Ô∏è Intenci√≥n inv√°lida "${intent}", usando fallback`);
        return this.fallbackClassification(message);
      }
      
      return {
        intent,
        confidence: 0.85, // Alta confianza para LLM
        processingTime,
        method: 'llm',
        model: this.model
      };
      
    } catch (error) {
      console.error(`[LLM_CLASSIFIER] ‚ùå Error en clasificaci√≥n LLM:`, error);
      
      // Fallback a clasificaci√≥n por regex
      return this.fallbackClassification(message);
    }
  }

  /**
   * Clasificaci√≥n de fallback usando regex (backup)
   */
  fallbackClassification(message) {
    console.log(`[LLM_CLASSIFIER] üîÑ Usando clasificaci√≥n fallback para: "${message}"`);
    
    const text = message.toLowerCase().trim();
    
    // Conversacional
    if (/hola|hi|hello|buenos d√≠as|buenas tardes|c√≥mo est√°s|qu√© tal|gracias|adi√≥s/i.test(text)) {
      return { intent: 'casual_conversation', confidence: 0.7, method: 'fallback' };
    }
    
    if (/qu√© puedes hacer|qui√©n eres|para qu√© sirves|qu√© haces|ayuda|help/i.test(text)) {
      return { intent: 'capability_question', confidence: 0.7, method: 'fallback' };
    }
    
    // Ag√©ntico
    if (/buscar en twitter|twitter|nitter|trending|hashtag|viral/i.test(text)) {
      return { intent: 'nitter_search', confidence: 0.6, method: 'fallback' };
    }
    
    if (/analiza.*twitter|sentimiento.*twitter/i.test(text)) {
      return { intent: 'twitter_analysis', confidence: 0.6, method: 'fallback' };
    }
    
    if (/perfil.*twitter|usuario.*twitter/i.test(text)) {
      return { intent: 'twitter_profile', confidence: 0.6, method: 'fallback' };
    }
    
    if (/busca informaci√≥n|investiga sobre|b√∫squeda web/i.test(text)) {
      return { intent: 'web_search', confidence: 0.6, method: 'fallback' };
    }
    
    if (/codex|mis documentos|archivos personales/i.test(text)) {
      return { intent: 'search_codex', confidence: 0.6, method: 'fallback' };
    }
    
    if (/mis proyectos|proyectos activos/i.test(text)) {
      return { intent: 'search_projects', confidence: 0.6, method: 'fallback' };
    }
    
    if (/analiza.*documento|resumen.*documento/i.test(text)) {
      return { intent: 'analyze_document', confidence: 0.6, method: 'fallback' };
    }
    
    // Default
    return { intent: 'unknown', confidence: 0.3, method: 'fallback' };
  }

  /**
   * Genera respuesta conversacional usando LLM
   */
  async generateConversationalResponse(message, intent, conversationHistory = []) {
    try {
      const contextPrompt = this.getConversationalPrompt(intent);
      
      const response = await this.openai.chat.completions.create({
        model: this.model,
        messages: [
          { role: 'system', content: contextPrompt },
          { role: 'user', content: message }
        ],
        max_tokens: 150,
        temperature: 0.7
      });

      return response.choices[0]?.message?.content?.trim();
      
    } catch (error) {
      console.error(`[LLM_CLASSIFIER] ‚ùå Error generando respuesta conversacional:`, error);
      return this.getFallbackResponse(intent);
    }
  }

  /**
   * Prompts para respuestas conversacionales
   */
  getConversationalPrompt(intent) {
    const prompts = {
      casual_conversation: `Eres Vizta, un asistente de IA amigable y profesional especializado en an√°lisis de redes sociales y gesti√≥n de documentos. Responde de manera natural y c√°lida a saludos y conversaci√≥n casual. Mant√©n un tono profesional pero cercano. M√°ximo 50 palabras.`,
      
      capability_question: `Eres Vizta, un asistente especializado en an√°lisis de redes sociales y gesti√≥n de documentos. Explica detalladamente que puedes hacer:

FUNCIONES PRINCIPALES:
üê¶ Twitter/X: Buscar tweets, analizar tendencias, sentimientos, perfiles de usuarios
üìö Codex Personal: Consultar documentos personales del usuario
üìã Proyectos: Revisar estado y informaci√≥n de proyectos activos  
üîç Investigaci√≥n: Buscar informaci√≥n actualizada en internet

Incluye ejemplos espec√≠ficos de comandos que el usuario puede usar. Tono profesional y √∫til. M√°ximo 120 palabras.`,
      
      help_request: `Eres Vizta, un asistente de IA especializado. Ofrece ayuda espec√≠fica con ejemplos concretos de comandos que el usuario puede usar:
- Ejemplos de b√∫squedas en Twitter
- Comandos para consultar el Codex
- C√≥mo revisar proyectos
- Tipos de investigaci√≥n disponibles

S√© espec√≠fico y pr√°ctico. M√°ximo 100 palabras.`,
      
      small_talk: `Eres Vizta, responde de manera amigable pero profesional a charla casual. Mant√©n el foco en c√≥mo puedes ayudar al usuario con an√°lisis de redes sociales, documentos o investigaci√≥n. M√°ximo 50 palabras.`
    };
    
    return prompts[intent] || prompts.small_talk;
  }

  /**
   * Respuestas de fallback predefinidas
   */
  getFallbackResponse(intent) {
    const responses = {
      casual_conversation: "¬°Hola! üëã Soy Vizta, tu asistente inteligente. ¬øEn qu√© puedo ayudarte hoy?",
      
      capability_question: `Puedo ayudarte con:
üê¶ **Twitter/X**: Buscar tweets, analizar tendencias, perfiles
üìö **Tu Codex**: Consultar tus documentos personales
üìã **Proyectos**: Revisar el estado de tus proyectos activos
üîç **Investigaci√≥n**: Buscar informaci√≥n actualizada en internet

¬øQu√© necesitas hacer?`,

      help_request: `¬°Por supuesto! Te puedo ayudar con:

**Comandos de ejemplo:**
‚Ä¢ "busca en twitter sobre guatemala"
‚Ä¢ "analiza el sentimiento sobre las elecciones"
‚Ä¢ "busca en mi codex informaci√≥n sobre migraci√≥n"
‚Ä¢ "¬øcu√°les son mis proyectos activos?"
‚Ä¢ "investiga sobre la econom√≠a guatemalteca"

¬øQu√© te gustar√≠a hacer?`,

      small_talk: "¬°Gracias! ¬øHay algo espec√≠fico en lo que pueda asistirte hoy? Puedo ayudarte con an√°lisis de redes sociales, consultas de documentos o investigaci√≥n."
    };
    
    return responses[intent] || "¬øEn qu√© puedo ayudarte?";
  }
}

module.exports = new LLMIntentClassifier(); 