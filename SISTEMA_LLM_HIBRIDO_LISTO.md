# Sistema LLM H√≠brido de Vizta - Listo para Pruebas

## üéØ ¬øQu√© hemos implementado?

Hemos creado un **sistema h√≠brido LLM + Ag√©ntico** que permite a Vizta funcionar tanto como:
- **üí¨ Chatbot conversacional** para saludos, preguntas generales y charla casual
- **ü§ñ Orquestador ag√©ntico** para tareas complejas que requieren Laura, Robert o ambos

---

## üß† Arquitectura del Sistema

### 1. **Clasificador LLM Principal**
- **Archivo**: `ExtractorW/server/services/agents/vizta/helpers/llmIntentClassifier.js`
- **Funci√≥n**: Usa OpenAI GPT-3.5-turbo para clasificar intenciones con alta precisi√≥n
- **Fallback**: Regex inteligente si el LLM falla
- **Latencia**: ~200-500ms para clasificaci√≥n

### 2. **Handlers Especializados**
- **Archivo**: `ExtractorW/server/services/agents/vizta/agentHandlers.js`
- **Laura Handlers**: Twitter, an√°lisis social, b√∫squedas web
- **Robert Handlers**: Codex, proyectos, documentos personales
- **Mixed Handlers**: An√°lisis que requieren m√∫ltiples agentes

### 3. **Motor H√≠brido de Vizta**
- **Archivo**: `ExtractorW/server/services/agents/vizta/index.js`
- **Decisi√≥n inteligente**: Modo conversacional vs ag√©ntico
- **Orquestaci√≥n**: Delega tareas seg√∫n la intenci√≥n detectada

---

## üöÄ C√≥mo Probar el Sistema

### **Opci√≥n 1: Prueba R√°pida**
```bash
cd ExtractorW
node test-llm-system.js --quick
```

### **Opci√≥n 2: Suite Completa**
```bash
cd ExtractorW
node test-llm-system.js
```

### **Opci√≥n 3: Modo Interactivo**
```bash
cd ExtractorW
node test-llm-system.js --interactive
```

---

## üí¨ Ejemplos de Uso

### **Modo Conversacional (Vizta responde directamente)**

| Consulta del Usuario | Intenci√≥n Detectada | Respuesta Esperada |
|---------------------|-------------------|-------------------|
| `"hola"` | `casual_conversation` | Saludo amigable de Vizta |
| `"¬øqu√© puedes hacer?"` | `capability_question` | Lista de capacidades |
| `"ayuda"` | `help_request` | Gu√≠a de uso |
| `"gracias"` | `casual_conversation` | Respuesta cordial |

### **Modo Ag√©ntico (Delega a agentes especializados)**

| Consulta del Usuario | Intenci√≥n Detectada | Agente Usado | Acci√≥n |
|---------------------|-------------------|-------------|--------|
| `"busca en twitter sobre guatemala"` | `nitter_search` | **Laura** | B√∫squeda en Twitter |
| `"analiza sentimiento de las elecciones"` | `twitter_analysis` | **Laura** | An√°lisis de sentimiento |
| `"busca el perfil de @presidente"` | `twitter_profile` | **Laura** | B√∫squeda de perfil |
| `"investiga sobre econom√≠a guatemalteca"` | `web_search` | **Laura** | B√∫squeda web |
| `"busca en mi codex sobre migraci√≥n"` | `search_codex` | **Robert** | B√∫squeda en documentos |
| `"¬øcu√°les son mis proyectos activos?"` | `search_projects` | **Robert** | Consulta de proyectos |
| `"analiza este documento"` | `analyze_document` | **Robert** | An√°lisis de documento |
| `"compara mis proyectos con Twitter"` | `mixed_analysis` | **Laura + Robert** | An√°lisis mixto |

---

## üîß Integraci√≥n con Frontend

### **Desde ViztalChat.tsx**
El frontend ya est√° configurado para trabajar con este sistema. Las respuestas incluyen:

```json
{
  "success": true,
  "response": {
    "agent": "Vizta",
    "message": "¬°Hola! üëã Soy Vizta, tu asistente inteligente. ¬øEn qu√© puedo ayudarte hoy?",
    "type": "conversational",
    "intent": "casual_conversation",
    "mode": "conversational"
  },
  "metadata": {
    "intent": "casual_conversation",
    "intentConfidence": 0.85,
    "intentMethod": "llm",
    "mode": "conversational",
    "processingTime": 347
  }
}
```

---

## üìä Monitoreo y M√©tricas

### **Logs del Sistema**
El sistema genera logs detallados:

```bash
[LLM_CLASSIFIER] üß† Clasificando: "hola"
[LLM_CLASSIFIER] ‚úÖ Intenci√≥n detectada: "casual_conversation" (245ms)
[VIZTA] üí¨ Modo conversacional activado para: casual_conversation
```

### **M√©tricas de Rendimiento**
- **Tiempo de clasificaci√≥n LLM**: 200-500ms
- **Tiempo de respuesta conversacional**: 300-800ms
- **Tiempo de respuesta ag√©ntica**: 1-5s (seg√∫n complejidad)
- **Fallback a regex**: <50ms

---

## üéõÔ∏è Configuraci√≥n

### **Variables de Entorno Requeridas**
```bash
OPENAI_API_KEY=tu_api_key_de_openai
```

### **Configuraci√≥n del LLM**
En `llmIntentClassifier.js`:
```javascript
this.model = 'gpt-3.5-turbo';        // Modelo r√°pido y eficiente
this.maxTokens = 50;                 // Para clasificaci√≥n
this.temperature = 0.1;              // Baja para consistencia
```

---

## üß™ Casos de Prueba Validados

### **‚úÖ Conversacionales**
- [x] Saludos b√°sicos ("hola", "buenos d√≠as")
- [x] Preguntas sobre capacidades 
- [x] Solicitudes de ayuda
- [x] Agradecimientos y despedidas
- [x] Charla casual

### **‚úÖ Ag√©nticos - Laura**
- [x] B√∫squedas en Twitter por keywords
- [x] An√°lisis de sentimiento
- [x] B√∫squedas de perfiles espec√≠ficos
- [x] Investigaci√≥n web general

### **‚úÖ Ag√©nticos - Robert**
- [x] B√∫squedas en Codex personal
- [x] Consultas de proyectos
- [x] An√°lisis de documentos

### **‚úÖ Casos Edge**
- [x] Texto sin sentido ‚Üí fallback inteligente
- [x] Queries vac√≠os ‚Üí manejo elegante
- [x] Multi-intenci√≥n ‚Üí priorizaci√≥n inteligente
- [x] Ingl√©s/espa√±ol ‚Üí detecci√≥n correcta

---

## üîÑ Flujo de Procesamiento

```mermaid
graph TD
    A[Usuario env√≠a mensaje] --> B[LLM Classifier]
    B --> C{¬øIntenci√≥n conversacional?}
    C -->|S√≠| D[Modo Conversacional]
    C -->|No| E[Modo Ag√©ntico]
    D --> F[LLM genera respuesta]
    E --> G{¬øQu√© agente?}
    G -->|Laura| H[Twitter/Web Search]
    G -->|Robert| I[Codex/Projects]
    G -->|Mixto| J[Laura + Robert]
    F --> K[Respuesta al usuario]
    H --> K
    I --> K
    J --> K
```

---

## üö® Troubleshooting

### **Error: API Key de OpenAI**
```bash
Error: You didn't provide an API key
```
**Soluci√≥n**: Configura `OPENAI_API_KEY` en tu `.env`

### **Error: Timeout de LLM**
```bash
[LLM_CLASSIFIER] ‚ùå Error en clasificaci√≥n LLM: timeout
```
**Soluci√≥n**: El sistema autom√°ticamente usa fallback a regex

### **Error: Agente no responde**
```bash
[VIZTA] ‚ùå Error en modo ag√©ntico para nitter_search
```
**Soluci√≥n**: Verifica que Laura/Robert est√©n funcionando correctamente

---

## üéØ Pr√≥ximos Pasos Recomendados

### **1. Pruebas de Integraci√≥n**
```bash
# Prueba conversacional b√°sica
curl -X POST http://localhost:3001/api/vizta-chat/query \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer tu_token" \
  -d '{"message": "hola"}'

# Prueba ag√©ntica
curl -X POST http://localhost:3001/api/vizta-chat/query \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer tu_token" \
  -d '{"message": "busca en twitter sobre guatemala"}'
```

### **2. Monitoreo de Producci√≥n**
- Implementar m√©tricas de latencia por tipo de intenci√≥n
- Crear alertas para fallbacks frecuentes de LLM
- Monitorear accuracy de clasificaci√≥n de intenciones

### **3. Optimizaciones**
- Cachear clasificaciones de intenciones comunes
- Implementar fine-tuning del modelo de clasificaci√≥n
- Agregar m√°s patrones de fallback

---

## ‚úÖ ¬°Sistema Listo!

El sistema LLM h√≠brido est√° **completamente funcional** y listo para:

1. **Pruebas desde backend** con `test-llm-system.js`
2. **Integraci√≥n con frontend** via ViztalChat
3. **Producci√≥n** con monitoreo adecuado

El flujo es:
**Usuario** ‚Üí **LLM Classifier** ‚Üí **Modo Conversacional/Ag√©ntico** ‚Üí **Respuesta Inteligente**

**¬°Adelante, pru√©balo! üöÄ** 